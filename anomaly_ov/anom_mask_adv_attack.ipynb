{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02596ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    0: \"./inpainting_dataset\",  # parent folder expected to contain 'inpainted' and 'masks'\n",
    "    1: \"./trends_cv_data\"        # parent folder; should have 'inpainted' and 'masks'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17aae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrative = True\n",
    "# Parameters for anomaly scoring visualization\n",
    "model_checkpoint = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"  # or local path\n",
    "size = \"05b\"  # or \"7b\"\n",
    "images_dir = dataset[int(demonstrative)]  # this is now the PARENT folder\n",
    "pattern = \"*.*\"  # glob pattern\n",
    "max_side = 1024\n",
    "show_n = 12  # number of images to visualize (sorting functionality retained)\n",
    "sort_by_score = True  # if True, show top-N highest scores; else first N\n",
    "images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open json file\n",
    "\n",
    "import json\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "image_grid_pinpoints = config[\"image_grid_pinpoints\"]\n",
    "image_aspect_ratio = config[\"image_aspect_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os, torch\n",
    "from custom_anomaly_detector import OVAnomalyDetector, build_ov_anomaly_detector\n",
    "from utils_trends.image_processing import process_images\n",
    "from llava.model.anomaly_expert import AnomalyOV\n",
    "from utils_trends.vulnerability_map import predict_with_model, adversarial_recompute\n",
    "from utils_trends.visualization import visualize_anomaly_gradcam\n",
    "from utils_trends.metrics import compute_mask_anomaly_metrics, MetricsAverages\n",
    "\n",
    "# Derive inpainted and masks subfolders from parent\n",
    "inpainted_dir = os.path.join(images_dir, 'inpainted')\n",
    "masks_dir = os.path.join(images_dir, 'masks')\n",
    "if not os.path.isdir(inpainted_dir):\n",
    "    raise SystemExit(f\"Inpainted folder not found: {inpainted_dir}\")\n",
    "if not os.path.isdir(masks_dir):\n",
    "    print(f\"Warning: Masks folder not found: {masks_dir}. Mask visualization will be skipped.\")\n",
    "\n",
    "device = \"cpu\"\n",
    "# Load model\n",
    "model_name = \"llava_qwen\"\n",
    "\"\"\"\n",
    "vision_tower_name: str = \"google/siglip-so400m-patch14-384\",\n",
    "    anomaly_expert_path: str = \"./pretrained_expert_7b.pth\",\n",
    "    mm_projector_type: str = \"mlp2x_gelu\",\n",
    "    mm_hidden_size: int = 1152,\n",
    "    hidden_size: int = 896,\n",
    "    device: str = \"cuda\",\n",
    "    dtype: Any | None = None,\n",
    "    load_pretrained_projector: Any | None = None,\n",
    "    device_map: str = \"auto\"\n",
    "\"\"\"\n",
    "# model, image_processor = build_ov_anomaly_detector(\n",
    "#     vision_tower_name=\"google/siglip-so400m-patch14-384\",\n",
    "#     anomaly_expert_path='./pretrained_expert_05b.pth' if size != '7b' else './pretrained_expert_7b.pth',\n",
    "#     mm_projector_type=\"mlp2x_gelu\",\n",
    "#     mm_hidden_size=1152,\n",
    "#     hidden_size=896,\n",
    "#     device=device,\n",
    "#     dtype=torch.float32,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# model.save_checkpoint(\"./zs_checkpoint.pt\")\n",
    "# model, image_processor = OVAnomalyDetector.load_from_checkpoint(\"./checkpoints/checkpoint_epoch_1.pt\", device=device)\n",
    "model, image_processor = OVAnomalyDetector.load_from_checkpoint(\"./zs_checkpoint.pt\", device=device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Collect images ONLY from inpainted subfolder now\n",
    "image_paths = sorted([\n",
    "    p for p in glob(os.path.join(inpainted_dir, '**', pattern), recursive=True)\n",
    "    if os.path.isfile(p)\n",
    "])\n",
    "print(f\"Found {len(image_paths)} inpainted images in {inpainted_dir}\")\n",
    "if not image_paths:\n",
    "    raise SystemExit(\"No inpainted images found. Adjust parent folder or pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling removed: use ALL inpainted images\n",
    "image_paths_subset = image_paths  # keep variable name for downstream compatibility\n",
    "print(f\"Using all {len(image_paths_subset)} images for scoring and mask visualization.\")\n",
    "\n",
    "# Optional: quick check of mask correspondence count\n",
    "missing_masks = 0\n",
    "for p in image_paths_subset:\n",
    "    mp = os.path.join(masks_dir, os.path.basename(p))\n",
    "    if not os.path.isfile(mp):\n",
    "        missing_masks += 1\n",
    "if missing_masks:\n",
    "    print(f\"Warning: {missing_masks} masks missing out of {len(image_paths_subset)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d56e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_path:str, device, max_side=1024):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    # Load corresponding mask (single full-image mask)\n",
    "    mask_path = os.path.join(masks_dir, os.path.basename(img_path))\n",
    "    mask_img = Image.open(mask_path).convert('L') if os.path.isfile(mask_path) else None\n",
    "\n",
    "    if max(img.size) > max_side:\n",
    "        if img.width > img.height:\n",
    "            new_w = max_side\n",
    "            new_h = int(max_side * img.height / img.width)\n",
    "        else:\n",
    "            new_h = max_side\n",
    "            new_w = int(max_side * img.width / img.height)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    image_tensor = process_images([img], image_processor, image_aspect_ratio, image_grid_pinpoints).to(device)\n",
    "    mask_tensor = process_images([mask_img], image_processor, image_aspect_ratio, image_grid_pinpoints).to(device)\n",
    "    return image_tensor, mask_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly scores for ALL inpainted images now (sampling removed)\n",
    "import numpy as np\n",
    "scores = []\n",
    "metrics_results = []  # store per-image metrics\n",
    "break_at = 3\n",
    "\n",
    "# Metrics aggregator class\n",
    "\n",
    "\n",
    "metrics_agg_original = MetricsAverages()  # For original anomaly maps\n",
    "metrics_agg_output = MetricsAverages()    # For output difference maps\n",
    "\n",
    "# Attack hyperparameters\n",
    "attack_epsilon = 0.05  # norm bound\n",
    "attack_top_k = 0.50    # fraction for M_topk\n",
    "# ['fgsm', 'pgd', 'deepfool']\n",
    "attack_noise_mode = 'pgd'  # 'random' or 'structured'\n",
    "\n",
    "for idx, img_path in enumerate(image_paths_subset, 1):\n",
    "    image_tensor, mask_tensor = get_image(img_path, device)\n",
    "    print(f\"mask shape: {mask_tensor.shape}\")\n",
    "    W, H = image_tensor.shape[3:]\n",
    "\n",
    "    print(f\"width: {W}, height: {H}, device: {image_tensor.device}\")\n",
    "    with torch.no_grad():\n",
    "        score, attn_maps, anomaly_maps, vuln_map, image_tensor = predict_with_model(\n",
    "            model=model,\n",
    "            image_tensor=image_tensor,\n",
    "            device=device,\n",
    "            max_side=1024,\n",
    "            anomaly_map_size=(W,H),\n",
    "        )\n",
    "    print(f\"Processed image tensor shape: {image_tensor.shape}\")\n",
    "\n",
    "    # Compute metrics on ORIGINAL anomaly map\n",
    "    metrics_original = compute_mask_anomaly_metrics(\n",
    "        anomaly_maps=anomaly_maps,\n",
    "        mask_image=mask_tensor,\n",
    "        top_k=0.50,\n",
    "        aggregate='mean',\n",
    "        white_threshold=None,\n",
    "        inpainted_is_white=True,\n",
    "    )\n",
    "    metrics_agg_original.update(metrics_original)\n",
    "\n",
    "    # Adversarial attack + recompute anomaly map\n",
    "    attacked_image, anomaly_maps_adv, output_map, adv_score = adversarial_recompute(\n",
    "        model=model,\n",
    "        image_tensor=image_tensor,\n",
    "        original_anomaly_maps=anomaly_maps,\n",
    "        device=device,\n",
    "        epsilon=attack_epsilon,\n",
    "        top_k=attack_top_k,\n",
    "        noise_mode=attack_noise_mode,\n",
    "    )\n",
    "\n",
    "    # Compute metrics on OUTPUT difference map\n",
    "    metrics_output = compute_mask_anomaly_metrics(\n",
    "        anomaly_maps=output_map,\n",
    "        mask_image=mask_tensor,\n",
    "        top_k=0.50,\n",
    "        aggregate='mean',\n",
    "        white_threshold=None,\n",
    "        inpainted_is_white=True,\n",
    "    )\n",
    "    metrics_results.append((img_path, {'original': metrics_original, 'output': metrics_output}))\n",
    "    metrics_agg_output.update(metrics_output)\n",
    "\n",
    "    # Prepare metrics label with two decimals (showing both)\n",
    "    def f2(x):\n",
    "        return 'nan' if (x is None or (isinstance(x, float) and np.isnan(x))) else f\"{float(x):.2f}\"\n",
    "    \n",
    "    metrics_label = (\n",
    "        f\"Original: IoU={f2(metrics_original.get('iou_topk'))} Mass={f2(metrics_original.get('mass_frac'))} \"\n",
    "        f\"ROC={f2(metrics_original.get('roc_auc'))} PR={f2(metrics_original.get('pr_auc'))} | \"\n",
    "   \n",
    "        f\"Output Δ: IoU={f2(metrics_output.get('iou_topk'))} Mass={f2(metrics_output.get('mass_frac'))} \"\n",
    "        f\"ROC={f2(metrics_output.get('roc_auc'))} PR={f2(metrics_output.get('pr_auc'))} \"\n",
    "        f\"score={score:.4f} adv_Score={adv_score:.4f}\"\n",
    "    )\n",
    "    print(f\"Metrics for image {idx}:\")\n",
    "    print(f\"  Original anomaly: IoU={f2(metrics_original.get('iou_topk'))}, Mass={f2(metrics_original.get('mass_frac'))}, ROC={f2(metrics_original.get('roc_auc'))}, PR={f2(metrics_original.get('pr_auc'))}\")\n",
    "    print(f\"  Output Δ map:     IoU={f2(metrics_output.get('iou_topk'))}, Mass={f2(metrics_output.get('mass_frac'))}, ROC={f2(metrics_output.get('roc_auc'))}, PR={f2(metrics_output.get('pr_auc'))}\")\n",
    "\n",
    "    # Updated visualization with Adv Anomaly and Output |Δ| Map (single metrics label)\n",
    "    fig = visualize_anomaly_gradcam(\n",
    "        image_tensor,\n",
    "        anomaly_maps,\n",
    "        vuln_maps=vuln_map if len(vuln_map) > 0 else None,\n",
    "        mask_image=mask_tensor,\n",
    "        anomaly_maps_adv=anomaly_maps_adv,\n",
    "        output_maps=output_map,\n",
    "        alpha=0.5,\n",
    "        metrics_label=metrics_label,\n",
    "    )\n",
    "    if idx == break_at:\n",
    "        break\n",
    "    scores.append((img_path, score))\n",
    "    if idx % 20 == 0:\n",
    "        print(f\"[{idx}] {img_path} -> {score:.6f}\")\n",
    "\n",
    "print(f\"\\nScored {len(scores)} images\")\n",
    "scores_array = np.array([[s[0], s[1]] for s in scores], dtype=object)\n",
    "print(\"\\n=== Average Metrics ===\")\n",
    "print(\"Original anomaly maps:\", str(metrics_agg_original))\n",
    "print(\"Output difference maps:\", str(metrics_agg_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# # Load a test image\n",
    "# test_img_path = image_paths_subset[0] if image_paths_subset else image_paths[0]\n",
    "# test_img = Image.open(test_img_path).convert('RGB')\n",
    "\n",
    "# # Process the image\n",
    "# image_tensor = process_images([test_img], image_processor, model.config)\n",
    "# print(f\"Image tensor shape: {image_tensor.shape}\")\n",
    "\n",
    "# # Get attention maps and anomaly maps from the model\n",
    "# _, _, final_preds, attn_maps, anomaly_maps = model.get_anomaly_fetures_from_images(\n",
    "#     image_tensor, \n",
    "#     with_attention_map=True,\n",
    "#     anomaly_map_size=(350, 350)\n",
    "# )\n",
    "\n",
    "# print(f\"Attention maps shape: {attn_maps.shape}\")\n",
    "# print(f\"Anomaly maps shape: {anomaly_maps.shape}\")\n",
    "\n",
    "# # Visualize everything\n",
    "# fig = visualize_attention_and_anomaly_maps(image_tensor, attn_maps, anomaly_maps)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2c8cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd67ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebe019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6396fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advCLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
